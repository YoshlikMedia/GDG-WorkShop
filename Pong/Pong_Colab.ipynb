{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZDInOWWw2ju",
        "outputId": "d094b751-85fc-4b26-fdf1-7905a154e150"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "\n",
        "!pip install atari-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8eZlq2bFrs"
      },
      "source": [
        "Consult https://github.com/thinkingparticle/deep_rl_pong_keras/blob/master/reinforcement_learning_pong_keras_policy_gradients.ipynb \n",
        "\n",
        "Especially for running visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XQkIssh7MEQ",
        "outputId": "004d960a-cda9-471a-cd61-b295c1570fdf"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS  rars/\n",
        "!mv ROMS  rars/\n",
        "!python -m atari_py.import_roms rars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "amPEhp5L8QYx",
        "outputId": "3ca6ee43-9b99-4029-c4d9-3b63fa4fd11c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uyjGZhfCW893"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# gym initialization\n",
        "env = gym.make(\"Pong-v4\")\n",
        "observation = env.reset()\n",
        "prev_input = None\n",
        "\n",
        "# Declaring the two actions that can happen in Pong for an agent, move up or move down\n",
        "# Decalring 0 means staying still. Note that this is pre-defined specific to package.\n",
        "UP_ACTION = 2\n",
        "DOWN_ACTION = 3\n",
        "\n",
        "# Hyperparameters. Gamma here allows you to measure the effect of future events\n",
        "gamma = 0.99\n",
        "\n",
        "# initialization of variables used in the main loop\n",
        "x_train, y_train, rewards = [],[],[]\n",
        "reward_sum = 0\n",
        "episode_nb = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "afuncmsNXksR",
        "outputId": "ee599bb2-37ce-4400-e077-aa44bd112690"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make(\"Pong-v4\")  # environment info\n",
        "observation = env.reset()\n",
        "# The ball is released after 20 frames\n",
        "for i in range(22):\n",
        "\n",
        "  if i > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "\n",
        "  observation, _, _, _ = env.step(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "__1EketPX5Dg"
      },
      "outputs": [],
      "source": [
        "#Preprocessing function\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195]  # crop\n",
        "  I = I[::2, ::2, 0]  # downsample by factor of 2\n",
        "  I[I == 144] = 0  # erase background (background type 1)\n",
        "  I[I == 109] = 0  # erase background (background type 2)\n",
        "  I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "QPGA-G6gXyXw",
        "outputId": "8a6c84bb-a50c-49e8-835e-81def0bfaea8"
      },
      "outputs": [],
      "source": [
        "#Show preprocessed\n",
        "obs_preprocessed = prepro(observation).reshape(80,80)\n",
        "plt.imshow(obs_preprocessed, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkWXVpJ8ORV"
      },
      "source": [
        "Original Floyd Approach at https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/\n",
        "\n",
        "Also consider https://github.com/mrahtz/tensorflow-rl-pong/blob/master/pong.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "cmf8bs2gynYS",
        "outputId": "50f58396-bd1c-47d4-8758-f0dfd18708f3"
      },
      "outputs": [],
      "source": [
        "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
        "def discount_rewards(r, gamma):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  r = np.array(r)\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add = 0\n",
        "  # we go from last reward to first one so we don't have to do exponentiations\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
        "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
        "    discounted_r[t] = running_add\n",
        "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
        "  discounted_r /= np.std(discounted_r) #idem using standar deviation\n",
        "  return discounted_r\n",
        "\n",
        "\"\"\"\n",
        "The function works thusly: negative rewards are spread to the frames before our model missed the ball (before the -1.0 in reward), idem for the positive.\n",
        "\n",
        "Essentially, we set the reward of actions taken before each reward, similar to the reward obtained.\n",
        "\n",
        "for example if we got reward +1 at time 200, we say that reward of time 199 is +0.99, reward of time 198 is +0.98 and so on.\n",
        "\n",
        "But note that the list created will have all 21 points of interactions done. So you have multiple lives here within a single term. Have to work out how this method works exactly\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg1X1rK8w-eo",
        "outputId": "a8bc932b-dc4f-4ac7-9d16-c95d4e2fa39b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 200)               1280200   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,401\n",
            "Trainable params: 1,280,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules from keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# creates a generic neural network architecture\n",
        "\n",
        "\"\"\"\n",
        "The 80 * 80 input dimension comes from the pre-processing of the raw pixels made by Karpathy (the only important pixels are the balls and the paddle)\n",
        "Input here represents the difference in pixels betewen one frame and another, giving you direction of agents and ball. Encoded in Karpathy's own preprocessing functions\n",
        "\n",
        "TODO, try adding a 400 layer infront of the 200 unit layer\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# hidden layer takes a pre-processed frame as input, and has 200 units. Simple layer architectur of 200 x1, 1x1\n",
        "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "# output layer - we use a Sigmoid here, in order to get a 0, or 1 value to represent ACTION UP\n",
        "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
        "\n",
        "# compile the model using traditional Machine Learning losses and optimizers\n",
        "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLPFOog_w_A-",
        "outputId": "425df958-4506-4d78-af22-1e90e6c197f5"
      },
      "outputs": [],
      "source": [
        "history = []\n",
        "observation = env.reset()\n",
        "prev_input = None\n",
        "# main training loop\n",
        "while (True):\n",
        "\n",
        "    \"\"\"\n",
        "    Start by preprocessing the observation frame, and then doing the difference with the previous frame. Naturally if frame 1 then we subtract by zeros\n",
        "    \n",
        "    X here is the frame-frame difference, Y is the next action (kind of like an RNN). We are trying to predict the next action given two observations.\n",
        "    \"\"\"\n",
        "    cur_input = prepro(observation)\n",
        "    #print(len(cur_input)) - Sanity Check reasons only\n",
        "    \n",
        "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
        "    prev_input = cur_input\n",
        "    \n",
        "    # forward the policy network and sample action according to the probability distribution\n",
        "    \"\"\"\n",
        "    Running model.predict to know what the current model thinks about the probability of doing the UP_ACTION, given the current frame setting.\n",
        "    \n",
        "    Double check size and shape of the array here im pretty sure its for bias term?\n",
        "    Keras requires a third?? dimension perhaps hre\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
        "    # proba=model.predict(np.expand_dims(x.reshape(80,80), axis=0)) 2D model stuff DELETE\n",
        "    \n",
        "    \n",
        "    #Intorucing another probability distirubtion here, not sure\n",
        "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
        "    y = 1 if action == 2 else 0 # 0 and 1 are our labels\n",
        "\n",
        "    # log the input and label to train later\n",
        "    x_train.append(x)\n",
        "    y_train.append(y)\n",
        "\n",
        "    # do one step in our environment - This is returned by our environment in OpenAI gym.  \n",
        "    observation, reward, done, info = env.step(action)\n",
        "    #Note how AT EACH STEP A REWARD IS CALCULATED. THIS IS NOT EACH GAME BUT EACH FRAME-FRAME DIFFERENCE.  MOST OF THE TIME THIS IS 0\n",
        "    #THESE REWARDS ARE USED TO ENCOURAGE OR DISCOURAGE MOVEMENTS\n",
        "    rewards.append(reward)\n",
        "    reward_sum += reward\n",
        "    \n",
        "    \"\"\"\n",
        "    rewards : to each frame (x_train[frame_number]) and action (y_train[frame_number]) \n",
        "    is associated a reward (-1 if it missed the ball, 0 if nothing happens, and 1 if opponent misses the ball), so we get for instance the following array:\n",
        "    \"\"\"\n",
        "    \n",
        "    # end of an episode - The GYM also invokes DONE automatically. Invoked when one player reaches 21\n",
        "    if done:\n",
        "        \n",
        "        history.append(reward_sum)\n",
        "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
        "        if episode_nb>=3000 and reward_sum >=-12:\n",
        "          break\n",
        "        else:\n",
        "          \n",
        "        \n",
        "          # increment episode number\n",
        "          episode_nb += 1\n",
        "        \n",
        "          # training\n",
        "          model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
        "        \n",
        "          \"\"\"\n",
        "          If an action leads to a positive reward, it tunes the weights of the neural network so it keeps on predicting this winning action.\n",
        "          Otherwise, it tunes them in the opposite way\n",
        "\n",
        "\n",
        "          The function discount_rewards transforms the list of rewards so that even actions that remotely lead to positive rewards are encouraged. THIS IS IMPORTANT\n",
        "          OTHERWISE WE WOULD SIMPLY BE TRYING TO REPLICATE RANDOM MOVEMENTS\n",
        "\n",
        "          sample_weights is used to provide a weight for each training sample. \n",
        "          That means that you should pass a 1D array with the same number of elements as your training samples (indicating the weight for each of those samples. NOTE THIS IS NOT CLASS WEIGHTS\n",
        "          \"\"\"\n",
        "                                                             \n",
        "          # Reinitialization\n",
        "          x_train, y_train, rewards = [],[],[]\n",
        "          observation = env.reset()\n",
        "          reward_sum = 0\n",
        "          prev_input = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZWWoejVZHS_"
      },
      "outputs": [],
      "source": [
        "#Plot results - remember to call keyboard interrupt before this\n",
        "plt.plot(history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuNhjvkNdvpe"
      },
      "outputs": [],
      "source": [
        "#To Evaluate model on OpenAI gym, we will record a video via Ipython display\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMAixKqcd-E8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuH8KEiHaGRa"
      },
      "outputs": [],
      "source": [
        "#Evaluate model on openAi GYM\n",
        "#To do this consult https://github.com/thinkingparticle/deep_rl_pong_keras/blob/master/reinforcement_learning_pong_keras_policy_gradients.ipynb\n",
        "env = wrap_env(gym.make('Pong-v0'))\n",
        "observation = env.reset()\n",
        "new_observation = observation\n",
        "prev_input = None\n",
        "done = False\n",
        "while True:\n",
        "  if True:\n",
        "\n",
        "    #set input to network to be difference image\n",
        "    cur_input = prepro(observation)\n",
        "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
        "    prev_input = cur_input\n",
        "\n",
        "    # Sample an action (policy)\n",
        "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
        "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
        "\n",
        "    env.render()\n",
        "    # Return action to environment and extract\n",
        "    #next observation, reward, and status\n",
        "    observation = new_observation\n",
        "    new_observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "      #observation = env.reset()\n",
        "      break\n",
        "\n",
        "env.close()\n",
        "show_video()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Pong_Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
